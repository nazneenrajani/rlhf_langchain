{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing requirements for fastjsonschema: [Errno 2] No such file or directory: '/Users/nazneenrajani/miniconda3/envs/autoeval/lib/python3.9/site-packages/fastjsonschema-2.15.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing requirements for fastjsonschema: [Errno 2] No such file or directory: '/Users/nazneenrajani/miniconda3/envs/autoeval/lib/python3.9/site-packages/fastjsonschema-2.15.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai -q\n",
    "!pip install langchain -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
    "from langchain.chains.conversation.memory import ConversationalBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" #HF enterprise a/c key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"LLM is a large language model dialog agent. It is designed to follow human instructions on a wide range os tasks from questions answering and summarization to code debugging and poetry composition. \n",
    "Please converse with LLM using natural language text inputs and rate the response you receive.\n",
    "\n",
    "{history}\n",
    "Human: {input}\n",
    "Agent:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], \n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlhf-langchain.createDataset import loadDataset, CustomLLMChain\n",
    "\n",
    "\n",
    "# When instantiating the LLMChain just use CustomLLMChain\n",
    "chatgpt_chain = CustomLLMChain(\n",
    "    llm=OpenAI(temperature=0), \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    memory=ConversationalBufferWindowMemory(k=2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new CustomLLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mLLM is a large language model dialog agent. It is designed to follow human instructions on a wide range os tasks from questions answering and summarization to code debugging and poetry composition. \n",
      "Please converse with LLM using natural language text inputs and rate the response you receive.\n",
      "\n",
      "\n",
      "Human: Write a poem in the style of Shakespeare but making questions like Socrates\n",
      "Agent:\u001b[0m\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "Unrecognized request argument supplied: request_timeout",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/nazneenrajani/workspace/rlhf/rlhf-langchain/human-feedback.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nazneenrajani/workspace/rlhf/rlhf-langchain/human-feedback.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output \u001b[39m=\u001b[39m chatgpt_chain\u001b[39m.\u001b[39;49mpredict(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mWrite a poem in the style of Shakespeare but making questions like Socrates\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nazneenrajani/workspace/rlhf/rlhf-langchain/human-feedback.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/humanfeedback/savePredictions.py:45\u001b[0m, in \u001b[0;36mCustomLLMChain.predict\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m    **kwargs: Keys to pass to prompt template.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m        completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# add new functionality\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# get the prediction\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n\u001b[1;32m     46\u001b[0m \u001b[39m# save the prediction\u001b[39;00m\n\u001b[1;32m     47\u001b[0m savePrediction(prediction)\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/langchain/chains/base.py:155\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_outputs(outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/langchain/chains/base.py:152\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    147\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    148\u001b[0m     inputs,\n\u001b[1;32m    149\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    153\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/langchain/chains/llm.py:86\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 86\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply([inputs])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/langchain/chains/llm.py:77\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[1;32m     76\u001b[0m     \u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[1;32m     78\u001b[0m     outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m     \u001b[39mfor\u001b[39;00m generation \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mgenerations:\n\u001b[1;32m     80\u001b[0m         \u001b[39m# Get the text of the top generated string.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/langchain/chains/llm.py:72\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIf `stop` is present in any inputs, should be present in all.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         )\n\u001b[1;32m     71\u001b[0m     prompts\u001b[39m.\u001b[39mappend(prompt)\n\u001b[0;32m---> 72\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/langchain/llms/base.py:79\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m     81\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/langchain/llms/base.py:76\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m     73\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/langchain/llms/openai.py:158\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    156\u001b[0m _keys \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcompletion_tokens\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprompt_tokens\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtotal_tokens\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    157\u001b[0m \u001b[39mfor\u001b[39;00m _prompts \u001b[39min\u001b[39;00m sub_prompts:\n\u001b[0;32m--> 158\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(prompt\u001b[39m=\u001b[39;49m_prompts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    159\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    160\u001b[0m     _keys_to_use \u001b[39m=\u001b[39m _keys\u001b[39m.\u001b[39mintersection(response[\u001b[39m\"\u001b[39m\u001b[39musage\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/openai/api_resources/completion.py:31\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     33\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:100\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     92\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39mAPIRequestor(\n\u001b[1;32m     93\u001b[0m     api_key,\n\u001b[1;32m     94\u001b[0m     api_base\u001b[39m=\u001b[39mapi_base,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     organization\u001b[39m=\u001b[39morganization,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[0;32m--> 100\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    101\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    102\u001b[0m     url,\n\u001b[1;32m    103\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    104\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    105\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    106\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    110\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)  \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/openai/api_requestor.py:122\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    105\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     request_id: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    112\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    113\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    114\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    115\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[1;32m    121\u001b[0m     )\n\u001b[0;32m--> 122\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/openai/api_requestor.py:329\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    322\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    323\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    324\u001b[0m         )\n\u001b[1;32m    325\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    326\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 329\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    330\u001b[0m             result\u001b[39m.\u001b[39;49mcontent, result\u001b[39m.\u001b[39;49mstatus_code, result\u001b[39m.\u001b[39;49mheaders, stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    331\u001b[0m         ),\n\u001b[1;32m    332\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    333\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/autoeval/lib/python3.9/site-packages/openai/api_requestor.py:362\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    360\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    361\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 362\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    363\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    364\u001b[0m     )\n\u001b[1;32m    365\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Unrecognized request argument supplied: request_timeout"
     ]
    }
   ],
   "source": [
    "output = chatgpt_chain.predict(input=\"Write a poem in the style of Shakespeare but making questions like Socrates\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n[Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f312122f3ed45eefb96eb2533558f359e32fabfee58d97447889ae824b25bd02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
